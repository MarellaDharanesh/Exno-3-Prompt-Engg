# Exno-3-Prompt-Engg

# Ex.No: 3 	Evaluation of 2024 Prompting Tools Across Diverse AI Platforms: 
ChatGPT, Claude, Bard, Cohere Command, and Meta 
### DATE: 28-04-25                                                                          
### REGISTER NUMBER : 212222240062
 
### Aim:
To compare the performance, user experience, and response quality of different AI platforms (ChatGPT, Claude, Bard, Cohere Command, and Meta) within a specific use case, such as summarizing text or answering technical questions. Generate a Prompt based output using different Prompting tools of 2024.

### Algorithm:
### Introduction

In the rapidly evolving field of Artificial Intelligence, particularly in the realm of natural language processing and generation, multiple AI tools and platforms have emerged, each offering unique capabilities, advantages, and specialties. Companies like OpenAI, Anthropic, Google, Cohere, and Meta have significantly contributed to advancing AI capabilities through their respective models. As users interact with these AI tools for purposes ranging from academic research, content creation, customer service, and technical problem solving, it becomes increasingly important to evaluate which platform best meets different categories of requirements.

Prompt engineering has become a core skill that determines how effectively a human can extract meaningful, high-quality outputs from an AI model. A well-crafted prompt can make the difference between a vague response and a highly insightful, actionable one. Thus, understanding how different AI platforms respond to identical prompts offers valuable insights into their internal architectures, training methodologies, and overall effectiveness.

 ### Need for Evaluation

 Evaluating AI platforms is crucial for multiple reasons. First, with a variety of options available, users must choose the most appropriate AI for their specific tasks to maximize efficiency and outcome quality. Second, different AI platforms have different training datasets, optimization goals, and bias-correction mechanisms, leading to considerable variability in outputs. Third, enterprises intending to integrate AI into their workflows must be assured of consistent, reliable, and ethically sound behavior from the AI systems they employ. This necessitates a structured, empirical comparison.

Furthermore, by examining response quality across parameters such as accuracy, clarity, depth, relevance, and language fluency, one can objectively assess the strengths and limitations of each model. This information not only aids current users but also guides future AI development and research.
 ### Theory of Prompt Engineering

 Prompt engineering is the process of designing and refining inputs (or prompts) given to an AI model in order to achieve specific, desired outputs. In essence, it is a technique where human users craft their queries or instructions carefully to align closely with the way the AI model interprets input.

A prompt can be as simple as a question or as complex as a multi-layered instruction. The structure, wording, and specificity of a prompt greatly influence the quality of the AI's response.

Components of Effective Prompting:
Precision: Clearly define what is being asked.

Context: Providing enough background for the AI to understand nuances.

Constraints: Limiting response style, length, or depth where necessary.

Iteration: Refining prompts based on AI feedback to improve output quality.

By mastering prompt engineering, users can significantly enhance the performance and relevance of AI outputs across various platforms.

### Algorithm for Evaluation

Step 1: Define the Use Case
First, a specific task that could be universally handled by all AI platforms is selected. In this experiment, the chosen task was to summarize a complex technical paragraph into a clear and concise list of five key bullet points. Summarization was selected because it requires comprehension, condensation, clarity, and relevance — capabilities that highlight an AI's overall language prowess.

Step 2: Create a Uniform Set of Prompts
A uniform, clearly worded prompt was designed to ensure that every AI model received exactly the same input. This uniformity is essential to maintain fairness and consistency in evaluation.

Step 3: Conduct the Experiment
Each platform was accessed individually under identical conditions using the same input prompt. Efforts were made to minimize variables such as device differences, internet speed, and time of day, which might otherwise affect response time and output behavior.

Step 4: Record the Outputs
All outputs were meticulously documented, including any noticeable lag times, system errors, or interface issues encountered during the interactions.

Step 5: Evaluate the Responses
Each AI-generated output was evaluated independently using five strict criteria: accuracy, clarity, depth, relevance, and overall language quality. Scores were assigned systematically.

Step 6: Comparative Analysis
Data collected from each AI platform was compared through tables, graphs, and narrative commentary to highlight differences, similarities, and unique characteristics.

Step 7: Summarize and Recommend
Finally, based on empirical findings, conclusions were drawn and recommendations were made regarding the best use cases for each platform.

### Flow Diagram of Experimental Process

![image](https://github.com/user-attachments/assets/067b0d11-85a4-4893-a088-d632f91561b9)

### Uniform Prompt Used

Prompt:
"Summarize the following paragraph into exactly 5 concise, informative bullet points that capture the main ideas clearly and completely."

### Execution Observations Table

![image](https://github.com/user-attachments/assets/d2921de7-b6ed-4961-a491-759b8fc789a1)

### Detailed Metrics for Response Evaluation

1. Accuracy
This refers to the correctness of the information presented by the AI in its response. Responses were checked to see if they preserved the factual integrity of the original paragraph.

2. Clarity
This measures how understandable the AI’s response is to the average user. Well-structured, grammatically correct sentences contributed to higher clarity scores.

3. Depth
This evaluates the extent to which the AI captured not just surface-level details but also underlying themes and nuanced points from the original paragraph.

4. Relevance
This metric assesses whether the AI stayed focused on the original topic without introducing unrelated or filler content.

5. Language Quality
This involves the fluency, grammar, style, and sophistication of the language used in the response.

### Performance Evaluation Table

![image](https://github.com/user-attachments/assets/d9c098f1-55c0-49cd-bb0f-2f3bee76bd38)

### Observations and Inferences

ChatGPT consistently produced the most comprehensive and human-like responses, offering detailed but slightly verbose summaries.

Claude focused on safe, ethically aligned outputs, sometimes at the expense of brevity or richness in content.

Bard demonstrated excellent real-time facts integration but lacked the same depth as ChatGPT or Claude in synthesizing information.

Cohere Command excelled in straightforward factual summarization but did not demonstrate creative rephrasing or nuanced understanding.

Meta AI provided well-balanced responses with conversational fluency, making it highly suitable for customer service or casual interactions.

### Use Case Suitability Table

![image](https://github.com/user-attachments/assets/229a8efe-25a5-40e8-8605-9db0cd94abbc)

### Limitations of the Study

In future studies, it would be beneficial to:

Include prompts in multiple languages.

Test performance across a wider range of tasks such as coding, translation, storytelling, and technical writing.

Analyze AI behavior under multi-turn conversational settings.

Assess platforms for bias, hallucination rates, and safety compliance.

### Result:
Through a comprehensive, methodologically sound experiment, the relative performance of ChatGPT, Claude, Bard, Cohere Command, and Meta AI in 2024 has been systematically evaluated.
Each platform demonstrated specific strengths and was best suited for particular types of tasks.
ChatGPT, with its highly detailed and structured responses, emerged as the overall top performer in summarization tasks. However, other platforms like Bard and Claude offer distinct advantages in scenarios requiring fast retrieval or high ethical compliance, respectively.
Thus the Prompting tools are executed and analysed sucessfully .

